---
title: 'Project 2: Data Mining, Classification, Prediction'
author: "SDS322E"
date: ''
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
  pdf_document:
    toc: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, fig.align = "center", warning = F, message = F,
tidy=TRUE, tidy.opts=list(width.cutoff=60), R.options=list(max.print=100))

class_diag <- function(score, truth, positive, cutoff=.5){

  pred <- factor(score>cutoff,levels=c("TRUE","FALSE"))
  truth <- factor(truth==positive, levels=c("TRUE","FALSE"))

  tab<-table(truth, pred)
  acc=sum(diag(tab))/sum(tab)
  sens=tab[1,1]/rowSums(tab)[1]
  spec=tab[2,2]/rowSums(tab)[2]
  ppv=tab[1,1]/colSums(tab)[1]

#CALCULATE F1
  f1=2*(sens*ppv)/(sens+ppv)
  
#CALCULATE EXACT AUC
  truth<-as.numeric(truth=="TRUE")
  ord<-order(score, decreasing=TRUE)
  score <- score[ord]; truth <- truth[ord]
  TPR=cumsum(truth)/max(1,sum(truth))
  FPR=cumsum(!truth)/max(1,sum(!truth))
  dup<-c(score[-1]>=score[-length(score)], FALSE)
  TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
  n <- length(TPR)
  auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )
  round(data.frame(acc,sens,spec,ppv,f1,ba=(sens+spec)/2,auc, row.names = "Metrics"),4)
}
```

# Mining, Classification, Prediction

Timi Adio oa5782

### Introduction 

One of the datasets I chose was a sample from "Penn World Table (9.1) Macroeconomic Data for Select Countries, 1950-2017" with variables: country, isocode, year, human capital, real gdp, and share of labor compensation. The other dataset I chose was "Yearly populations of countries from 1960 to 2017" with variables: country, country code 1, country code 2, year, and population. I will be analyzing the data from 2000-2017. I found both these datasets on https://vincentarelbundock.github.io/Rdatasets/datasets.html, which was on the Project 2 directions page. 

I chose these two datasets because "country" was a common variable of theirs. I wanted to see if there was correlation between population, or change in population, and different measures of economic health (human capital, real gdp, etc.). There are 1160 observations and 20 different countries, so 20 countries per year. I later add a binary variable, 'population_tf,' that'll represent whether or not observations have population values greater than the mean population of the entire dataset. There are 580 observations each for TRUE and FALSE observations. Human capital (hc), real gdp (rgdpna), and share of labor compensation (labsh) represent exactly what they say. 

```{R}
library(tidyverse)
# read your datasets in here, e.g., with read_csv()
countrypops <- read_csv("https://vincentarelbundock.github.io/Rdatasets/csv/gt/countrypops.csv")
pwt_sample <- read_csv("https://vincentarelbundock.github.io/Rdatasets/csv/stevedata/pwt_sample.csv")

# if your dataset needs tidying, do so here
cpops <- countrypops %>% rename(country=country_name) %>% rename(x=X1) %>% select(-country_code_2, -country_code_3, -x)
pwt <- pwt_sample %>% rename(x=X1)  %>% select(-isocode, -pop, -x)
pwt %>% left_join(cpops, by=c("year", "country")) %>% na.omit -> pwt
# any other code here
```

### Cluster Analysis

```{R}
library(cluster)
library(ggplot2)
# clustering code here
clust_dat <- pwt %>% select(3:6)
sil_width<-vector() #empty vector to hold mean sil width
for(i in 2:10){  
  kms <- kmeans(clust_dat,centers=i) #compute k-means solution for each k
  sil <- silhouette(kms$cluster,dist(clust_dat)) #get sil widths
  sil_width[i]<-mean(sil[,3]) #take averages (higher is better)
}
ggplot()+geom_line(aes(x=1:10,y=sil_width))+scale_x_continuous(name="k",breaks=1:10)

pwt_pam <- clust_dat %>% pam(k=2)
pwt_pam

library(GGally)
pwt %>% mutate(cluster=as.factor(pwt_pam$clustering)) %>% ggpairs(columns = c(3:7), aes(color=cluster))

```

Population showed the greatest difference between the two clusters, while human capital showed the least difference. Cluster 1 represents countries-year combos with low real gdp and population. Cluster 2 represents country-year combos with high real gdp and population. The number of clusters was based on the largest average silhouette width. Only population vs rgdpna has strong correlation (0.916), while rgdpna vs hc has fairly weak correlation (0.408). All others have very weak correlation, so weak it's safe to assume there is no correlation between the associated variables. 
    
### Dimensionality Reduction with PCA

```{R}
# PCA code here
pwt_pca <- pwt %>% select(-country, -year)
princomp(pwt_pca, cor=T) -> pca1
summary(pca1, loadings=T) #get PCA summary

eigval <-  pca1$sdev^2 #square to convert SDs to eigenvalues
varprop=round(eigval/sum(eigval), 2) #proportion of var explained by each PC

ggplot() + geom_bar(aes(y=varprop, x=1:4), stat="identity") + xlab("") + geom_path(aes(y=varprop, x=1:4)) + geom_text(aes(x=1:4, y=varprop, label=round(varprop, 2)), vjust=1, col="white", size=5) + scale_y_continuous(breaks=seq(0, .6, .2), labels = scales::percent) + scale_x_continuous(breaks=1:10)

round(cumsum(eigval)/sum(eigval), 2)
```

PC1 represents the gdp vs population axis because if a country-year combo is high in one, it tends ot be high in the other. PC2 represents the labor shortage vs human capital axis because if a country-year combo is high in one, it tends to be high in the other. PC3 represents the human capital vs population axis because if a country-year combo is high in human capital it tends to have a low population. PC4 represents the population vs gdp axis because if a country-year combo has a high population, it tends to have a low gdp. The first three principal components account for 98% of the variance.

###  Linear Classifier

```{R}
pwt <- pwt %>% mutate(population_tf = population > mean(population))

ggplot(pwt, aes(hc, rgdpna, color=population_tf)) + geom_point()

# linear classifier code here
logistic_fit <- glm(population_tf ~ hc + rgdpna, data=pwt, family="binomial")
prob_reg <- predict(logistic_fit, type="response")
class_diag(score=prob_reg, truth = pwt$population_tf, positive = "TRUE")

y <- pwt$population_tf
y_hat <- ifelse(prob_reg>0.5, "TRUE", "FALSE")
table(actual = y, predicted= y_hat)
```

```{R}
# cross-validation of linear classifier here
set.seed(322)
k=10

data<-sample_frac(pwt) #randomly order rows
folds <- rep(1:k, length.out=nrow(data)) #create folds

diags<-NULL

i=1
for(i in 1:k)
{
# create training and test sets
train<-data[folds!=i,] 
test<-data[folds==i,] 
test
truth<-test$population_tf
# train model
fit <- glm(population_tf ~ hc + rgdpna, data=train, family="binomial") ### SPECIFY THE LOGISTIC REGRESSION MODEL FIT TO THE TRAINING SET HERE

# test model
probs <- predict(fit, newdata=test, type = "response") ### GET PREDICTIONS FROM THE TRAINED MODEL ON THE TEST SET HERE

# get performance metrics for each fold
diags<-rbind(diags,class_diag(probs,truth, positive="TRUE")) 
}

#average performance metrics across all folds
summarize_all(diags,mean)
```

Based on the 0.9984 AUC, the model is performing well. There is no noticeable decrease in AUC when predicting out of sample, so this model shows no signs of overfitting. The 0.0002 difference is negligible. Looking at the confusion matrix, only 2.07% of the predictions were incorrect (false positive or false negative). This is another sign the model performed well.


### Non-Parametric Classifier

```{R}
library(caret)
# non-parametric classifier code here
knn_fit <- knn3(population_tf ~ hc + rgdpna, data=pwt)
#your code here
prob_knn <- predict(knn_fit, newdata=pwt)[,2]
class_diag(score=prob_knn, truth=pwt$population_tf, positive="TRUE")

y_hat <- ifelse(prob_knn>0.5, "TRUE", "FALSE")
table(actual = y, predicted= y_hat)
```

```{R}
# cross-validation of np classifier here
set.seed(322)
k=10

data<-sample_frac(pwt) #randomly order rows
folds <- rep(1:k, length.out=nrow(data)) #create folds

diags<-NULL

i=1
for(i in 1:k){
# create training and test sets
train<-data[folds!=i,] 
test<-data[folds==i,] 
truth<-test$population_tf

# train model
fit <- knn_fit <- knn3(population_tf=="TRUE" ~ hc + rgdpna, data=train) ### SPECIFY THE LOGISTIC REGRESSION MODEL FIT TO THE TRAINING SET HERE

# test model
probs <- predict(fit, newdata=test)[,2] ### GET PREDICTIONS FROM THE TRAINED MODEL ON THE TEST SET HERE

# get performance metrics for each fold
diags<-rbind(diags,class_diag(probs,truth,positive="TRUE")) }

#average performance metrics across all folds
summarize_all(diags,mean)
```

Based on the 0.9896 AUC, the model is performing well. There is an 0.032 decrease in AUC, which is insignificant, when predicting out of sample, so this model shows very little to no signs of overfitting. The linear model has slightly stronger cross-validation performance than the non-parametric model given its slightly higher AUC value (0.9984 vs 0.9896). Looking at the confusion matrix, only 5.26% of the predictions were incorrect (false positive or false negative). This is another sign the model performed well, but not a well as the linear model.


### Regression/Numeric Prediction

```{R}
# regression model code here
pwt_reg <- pwt %>% select(-country, -year, -population_tf)
fit<-lm(rgdpna~.,data=pwt_reg) #predict rgdpna from all other numerical variables
yhat <- predict(fit)
mean((pwt_reg$rgdpna-yhat)^2) #MSE calculation
```

```{R}
# cross-validation of regression model here
set.seed(1234)
k=5
data<-pwt[sample(nrow(pwt)),] 
folds<-cut(seq(1:nrow(pwt)),breaks=k,labels=F)
diags<-NULL
for(i in 1:k){
  train<-data[folds!=i,]
  test<-data[folds==i,]
  
  fit<-lm(rgdpna~.,data=train)
  
  yhat<-predict(fit,newdata=test)
  
  diags<-mean((test$rgdpna-yhat)^2) 
}
mean(diags) #average MSE across all folds
```

The MSE of the overall dataset is 103294463889. The average MSE across 5 testing folds is 35614228463. The MSE is very high, a sign that there may be overfitting. 

### Python 

```{R}
library(reticulate)
x <- "Hello"
```

```{python}
# python code here
x = "World"
y = "Final"
print(r.x + " " + x)

```

```{R}
library(reticulate)
y <- "Project"
cat(c(py$y, y))
```

I shared R's x object with Python by using "r.x" and shared Python's y object in r by using "py$y" in order to combine words from each language. I formed the phrase "Hello World" ("Hello" from R) in Python and the phrase "Final Project" ("Final" from Python) in R.

### Concluding Remarks

As expected, there was high correlation between population and rgdp. because the models created were testing population based on hc and rgdpna, it was not surprising to see that the models had little trouble with accuracy given the high correlation between rgdp and population. 




